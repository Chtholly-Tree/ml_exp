{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt     0\n",
      "Answer    19\n",
      "Target     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(Path('data').joinpath('train.csv'))\n",
    "train = train[['Prompt', 'Answer', 'Target']]\n",
    "# 统计各特征为null的数量\n",
    "print(train.isnull().sum())\n",
    "# 填充nan值\n",
    "train.loc[train['Answer'].isna(), 'Answer'] = 'NAN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Target</th>\n",
       "      <th>Text</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[INST] You are an AI assistant that helps peop...</td>\n",
       "      <td>Step-by-step reasoning process:\\n1. Randy spen...</td>\n",
       "      <td>0</td>\n",
       "      <td>[INST] You are an AI assistant that helps peop...</td>\n",
       "      <td>1043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[INST] You are an AI assistant. You will be gi...</td>\n",
       "      <td>What is the temperature at which hypothermia b...</td>\n",
       "      <td>0</td>\n",
       "      <td>[INST] You are an AI assistant. You will be gi...</td>\n",
       "      <td>2285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[INST] You are an AI assistant. You will be gi...</td>\n",
       "      <td>Answer: c) No. \\n\\nThe hypothesis is false bec...</td>\n",
       "      <td>0</td>\n",
       "      <td>[INST] You are an AI assistant. You will be gi...</td>\n",
       "      <td>1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[INST] You are an AI assistant. User will you ...</td>\n",
       "      <td>Prismatoid</td>\n",
       "      <td>0</td>\n",
       "      <td>[INST] You are an AI assistant. User will you ...</td>\n",
       "      <td>3519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[INST] You are an AI assistant. User will you ...</td>\n",
       "      <td>Case B</td>\n",
       "      <td>0</td>\n",
       "      <td>[INST] You are an AI assistant. User will you ...</td>\n",
       "      <td>1449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Prompt  \\\n",
       "0  [INST] You are an AI assistant that helps peop...   \n",
       "1  [INST] You are an AI assistant. You will be gi...   \n",
       "2  [INST] You are an AI assistant. You will be gi...   \n",
       "3  [INST] You are an AI assistant. User will you ...   \n",
       "4  [INST] You are an AI assistant. User will you ...   \n",
       "\n",
       "                                              Answer  Target  \\\n",
       "0  Step-by-step reasoning process:\\n1. Randy spen...       0   \n",
       "1  What is the temperature at which hypothermia b...       0   \n",
       "2  Answer: c) No. \\n\\nThe hypothesis is false bec...       0   \n",
       "3                                         Prismatoid       0   \n",
       "4                                             Case B       0   \n",
       "\n",
       "                                                Text   len  \n",
       "0  [INST] You are an AI assistant that helps peop...  1043  \n",
       "1  [INST] You are an AI assistant. You will be gi...  2285  \n",
       "2  [INST] You are an AI assistant. You will be gi...  1060  \n",
       "3  [INST] You are an AI assistant. User will you ...  3519  \n",
       "4  [INST] You are an AI assistant. User will you ...  1449  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_columns = ['Prompt', 'Answer']\n",
    "\n",
    "train['Text'] = train[text_columns].agg(' '.join, axis=1)\n",
    "train['len'] = train['Text'].apply(lambda x: len(x))\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  1031, 16021,  ...,     0,     0,     0])\n",
      "<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>\n"
     ]
    }
   ],
   "source": [
    "# 加载bert tokenize模型\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_len = 5000\n",
    "\n",
    "# bert使用示例\n",
    "encoding = tokenizer.encode_plus(\n",
    "    train['Text'][0],\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    return_token_type_ids=False,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "print(encoding['input_ids'].flatten())\n",
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, texts: List, labels: List, tokenizer: Any, max_len: int) -> None:\n",
    "        super().__init__()\n",
    "        self.texts = texts\n",
    "        self.lables = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx) -> Dict:\n",
    "        text = self.texts[idx]\n",
    "        label = self.lables[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text, \n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label).type(torch.LongTensor)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1031, 16021,  2102,  1033,  2017,  2024,  2019,  9932,  3353,\n",
       "         1012,  5310,  2097,  2017,  2507,  2017,  1037,  4708,  1012,  2115,\n",
       "         3125,  2003,  2000,  3143,  1996,  4708,  2004, 11633,  2135,  2004,\n",
       "         2017,  2064,  1012,  2096,  4488,  1996,  4708,  2228,  3357,  1011,\n",
       "         2011,  1011,  3357,  1998, 16114,  2115,  4084,  1012,  1031,  1013,\n",
       "        16021,  2102,  1033,  3335,  5143, 11586,  2098,  2041,  1010,  1998,\n",
       "         6871,  2000,  6814,  2008,  2016,  2018,  2053,  2051,  2000,  2228,\n",
       "         2127,  2016,  2001,  2006,  2014,  2126,  2188,  2247,  1996,  4064,\n",
       "         2346,  2461, 14855, 22426,  2012,  5408,  1051,  1005,  5119,  2008,\n",
       "         2305,  1012,  2059,  2009,  7537,  2014,  2000,  2131,  2041,  1997,\n",
       "         2014,  6174, 17980,  1998,  3328,  1012,  2045,  2001,  2019,  6728,\n",
       "        27581,  4231,  1010,  1996,  6565,  4564, 21439,  2091,  2000,  1996,\n",
       "         8575,  2020,  2035,  4462,  1998, 21666,  1010,  1998,  1996,  2139,\n",
       "        13390,  2869,  8964, 10424,  7585,  2094,  1996,  2346,  2007,  6918,\n",
       "         6281,  1012,  2055,  2014,  5112,  1996,  2307, 29435,  1999,  1037,\n",
       "        10478, 20334,  1999,  2029,  2210, 21934,  2721,  2003,  2275,  1010,\n",
       "         1998,  2009, 10650,  2014,  2013,  2054,  2018,  3047,  1010,  2061,\n",
       "         2008,  2016,  2071,  2298,  2012,  2009,  1998,  5390,  2041,  1012,\n",
       "         2016,  2941,  2106,  3713,  1010, 20490,  1999,  1996,  2210, 10531,\n",
       "         2006,  1996,  2346,  2073,  1996,  6821, 28478,  2015,  8587,  1999,\n",
       "         1996, 12217,  1010,  2021,  2200,  2659,  1010,  2061,  2008,  2014,\n",
       "         2616,  3062,  2461,  2014,  2130,  1999,  2008,  4223,  1010,  1998,\n",
       "         6684,  1037,  2139, 13390,  2099,  2001,  5204,  1012,  1005,  1045,\n",
       "         2097,  2025,  2175,  2085,  1010,  1005,  2016,  2056,  1012,  1005,\n",
       "         1045,  2097,  2994,  1998,  5382,  2008,  2002,  2003,  2178,  2450,\n",
       "         1005,  1055,  3129,  1012,  2008,  2323,  9526,  2033,  2065,  2505,\n",
       "         2097,  1011,  1011,  2000,  2156,  2032,  5129,  2011,  1996, 27550,\n",
       "         2015,  1997,  2496,  2166,  1010,  2008,  2785,  1997,  2496,  2166,\n",
       "         1012,  1045,  2097,  2994,  6229,  2016,  3310,  1998,  1037,  3481,\n",
       "        15864,  2044,  1012,  4661,  1010,  1045,  2215,  2000,  2156,  2014,\n",
       "         1011,  1011,  1045,  2215,  2000,  2156,  2129,  2521,  2016,  3310,\n",
       "         2460,  1012,  1005,  2016,  2001,  4333,  2005,  1037,  2617,  1010,\n",
       "         1998,  1996, 11986,  2209,  2588,  2014,  2868,  1997,  4251, 10911,\n",
       "         1012,  1005,  2002, 14977,  2205,  1010,  1005,  2016,  2056,  1025,\n",
       "         1005,  2002, 14977,  2205,  1010,  2021,  2002,  2987,  1005,  1056,\n",
       "         2113,  2009,  1010,  1998,  1045,  4872,  2017,  2028,  2518,  1010,\n",
       "        16974,  5143,  1010,  2017,  2180,  1005,  1056,  2393,  2032,  2000,\n",
       "         2424,  2041,  1012,  1998,  1999,  2274,  3134,  1045,  2097,  2175,\n",
       "         2185,  1998,  2681,  2026,  2293,  2073,  1045,  2179,  2009,  1011,\n",
       "         1011,  2006,  1037,  3137, 14399,  1999,  1996,  2690,  1997,  4021,\n",
       "          999,  1005,  3160,  1024,  1005,  2129,  2146,  2515,  3335,  5143,\n",
       "         2933,  2000,  2994,  1999,  1996, 10531,  1029,  1005,  3433,  1024,\n",
       "         1005,  1037,  2095,  1005,  2241,  2006,  1996, 20423,  1010,  2003,\n",
       "         1996,  3433,  2000,  1996,  3160,  2003, 25854,  2135,  6149,  1029,\n",
       "         4060,  2013,  1024,  1006,  1045,  1007,  1012,  2053,  1025,  1006,\n",
       "         2462,  1007,  1012,  2748,  1025,  1006,  3523,  1007,  1012,  2009,\n",
       "         9041,  2006,  1996,  6123,  1012,  3437,  1024,  1006,  2462,  1007,\n",
       "         1012,  2748,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train['Text'].tolist(),\n",
    "    train['Target'].tolist(),\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_len = 512\n",
    "train_dataset = QADataset(X_train, y_train, tokenizer, max_len)\n",
    "test_dataset = QADataset(X_test, y_test, tokenizer, max_len)\n",
    "\n",
    "# dataset使用示例\n",
    "train_dataset[0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建基于Bert的二分类神经网络\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, n_classes: int) -> None:\n",
    "       super(BertClassifier, self).__init__()\n",
    "       # 加载预训练模型\n",
    "       self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "       # 过拟合预防\n",
    "       self.dropout = nn.Dropout(p=0.3)\n",
    "       # 构建输出层\n",
    "       self.fc = nn.Linear(self.bert.config.hidden_size, n_classes) \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask= attention_mask)\n",
    "        pooled_output = outputs[1] # 获取 [CLS] 输出\n",
    "        output = self.dropout(pooled_output)\n",
    "        return self.fc(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "print(bert.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 16/1878 [00:15<29:34,  1.05it/s, loss=0.112] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     26\u001b[0m loop\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m loop\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BertClassifier(n_classes=2).to(device)\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1000):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
